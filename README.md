ğŸ“° News Analytics REST API

A cloud-native, analytics-driven RESTful API for ingesting, normalizing, and analyzing news data from multiple sources.
The system is designed as a data ingestion + analytics backend, supporting topic trends, source-level comparisons, and future bias analysis using an S3-based data lake and Athena.

â¸»

ğŸ“Œ Project Overview

Modern news analysis requires:
	â€¢	Aggregating data from multiple news providers
	â€¢	Normalizing inconsistent schemas
	â€¢	Storing data in a query-friendly analytics format
	â€¢	Enabling trend, frequency, and source-level insights

This project solves that by acting as the contract layer between:
	â€¢	External news APIs
	â€¢	A cloud-based data lake
	â€¢	Analytics and visualization tools

The API is built with Python + FastAPI, deployed as a stateless, scalable service, and designed to mirror real-world data engineering and backend workflows.

â¸»

ğŸ— Architecture Overview

High-level flow:
	1.	Scheduled or on-demand ingestion requests hit the REST API
	2.	The API fetches data from external news providers
	3.	Raw responses are normalized into a unified schema
	4.	Normalized data is stored in Amazon S3 (data lake)
	5.	Data can be queried using Amazon Athena for analytics
	6.	Aggregates are exposed via analytics endpoints or dashboards

Key design principle:

Treat the API as a production data service, not a script.

â¸»

ğŸ§° Tech Stack

Backend
	â€¢	Python 3.11
	â€¢	FastAPI â€“ async REST API framework
	â€¢	Pydantic â€“ schema validation & serialization
	â€¢	Uvicorn â€“ ASGI server

Cloud & Infrastructure
	â€¢	Amazon S3 â€“ raw & processed data lake
	â€¢	Amazon Athena â€“ SQL analytics on S3 data
	â€¢	AWS ECS (Fargate) â€“ containerized deployment
	â€¢	Terraform â€“ infrastructure as code
	â€¢	Docker â€“ containerization

Data & Analytics
	â€¢	JSON (raw ingestion)
	â€¢	Parquet (planned optimization)
	â€¢	Schema normalization for cross-source analysis

â¸»

ğŸ—‚ Repository Structure

news-analytics-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ ingest.py    # ingestion endpoints
â”‚   â”‚       â”œâ”€â”€ analytics.py # analytics endpoints
â”‚   â”‚       â””â”€â”€ health.py    # health checks
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ news_fetcher.py  # external API clients
â”‚   â”‚   â”œâ”€â”€ normalizer.py    # schema normalization
â”‚   â”‚   â”œâ”€â”€ s3_client.py    # S3 interactions
â”‚   â”‚   â””â”€â”€ athena.py       # Athena query execution
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ article.py      # unified article schema
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # environment configuration
â”‚   â”‚   â””â”€â”€ logging.py      # structured logging
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ time.py
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ ecs.tf
â”‚   â”œâ”€â”€ s3.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â””â”€â”€ variables.tf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

This structure intentionally mirrors real backend and data-platform repos.

â¸»

ğŸ”„ Data Normalization Strategy

Different news providers return different schemas.
To support analytics, all incoming data is normalized into a single canonical model:

Article(
    source: str,
    title: str,
    description: Optional[str],
    url: str,
    published_at: datetime,
    topic: Optional[str]
)

Why this matters:
	â€¢	Enables cross-source comparisons
	â€¢	Simplifies downstream analytics
	â€¢	Allows future bias & sentiment analysis
	â€¢	Decouples ingestion logic from analytics logic

â¸»

ğŸ“¥ API Endpoints

Health Check

GET /health

Used for load balancers and service monitoring.

â¸»

Ingest News Data

POST /api/v1/ingest?query=<topic>

What it does:
	â€¢	Fetches articles from external news APIs
	â€¢	Normalizes data into a unified schema
	â€¢	Stores raw normalized data in S3

Response:

{
  "status": "success",
  "count": 42,
  "s3_key": "raw/2026-02-01T21:14:32.json"
}


â¸»

Analytics (Planned / Expandable)

GET /api/v1/analytics/topics
GET /api/v1/analytics/sources

Provides:
	â€¢	Topic frequency counts
	â€¢	Source-level distributions
	â€¢	Time-based trends

These endpoints are backed by Athena SQL queries on S3 data.

â¸»

â˜ï¸ Data Lake Design (S3)

s3://news-datalake/
â”œâ”€â”€ raw/         # raw normalized JSON
â”œâ”€â”€ processed/   # cleaned / enriched data
â””â”€â”€ analytics/   # aggregates & query outputs

This layout mirrors industry-standard data lake architectures.

â¸»

ğŸš€ Local Development

Install dependencies

pip install -r requirements.txt

Run the API

uvicorn app.main:app --reload

API Docs

FastAPI auto-generates OpenAPI docs:

http://localhost:8000/docs


â¸»

ğŸ³ Docker Support

The service is fully containerized for local and cloud deployment.

docker build -t news-analytics-api .
docker run -p 8000:80 news-analytics-api


â¸»

ğŸŒ Cloud Deployment (AWS)

The API is designed for:
	â€¢	Stateless execution
	â€¢	Horizontal scaling
	â€¢	Managed infrastructure

Deployment stack:
	â€¢	ECS Fargate for compute
	â€¢	IAM roles for secure S3 & Athena access
	â€¢	Terraform for repeatable provisioning

â¸»

ğŸ“Š Analytics & Use Cases

This platform enables:
	â€¢	Tracking trending topics over time
	â€¢	Comparing coverage across news sources
	â€¢	Measuring publication volume by category
	â€¢	Supporting future bias, sentiment, and framing analysis

Example analytics questions:
	â€¢	Which outlets publish the most political content?
	â€¢	How does topic coverage shift week-to-week?
	â€¢	Are certain topics over-represented by specific sources?

â¸»

ğŸ”® Future Enhancements
	â€¢	Parquet conversion via AWS Glue
	â€¢	Sentiment analysis (NLP)
	â€¢	Source bias metrics
	â€¢	Scheduled ingestion via EventBridge
	â€¢	Dashboards via QuickSight or Grafana
	â€¢	Authentication (JWT / IAM)
	â€¢	Rate limiting and caching

â¸»

ğŸ¯ Why This Project Exists

This project was built to demonstrate:
	â€¢	Real REST API design (not toy endpoints)
	â€¢	Data engineering fundamentals
	â€¢	Cloud-native architecture
	â€¢	Analytics-first thinking
	â€¢	Production-ready structure and tooling

It reflects how modern backend systems support data pipelines, analytics, and decision-making, not just CRUD.

â¸»

If you want next, I can:
	â€¢	Rewrite this README shorter for recruiters
	â€¢	Add a system architecture diagram
	â€¢	Generate resume bullets directly from this README
	â€¢	Help you implement bias metrics properly

Just tell me how hard you want to go ğŸš€